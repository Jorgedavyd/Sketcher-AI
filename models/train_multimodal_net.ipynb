{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Imports \"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "from utils import *\n",
    "from multi_style import *\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 1\n",
    "STYLE_NAME = \"sketch\"\n",
    "LR = 1e-3\n",
    "NUM_EPOCHS = 1\n",
    "CONTENT_WEIGHTS = [1, 1, 1]\n",
    "STYLE_WEIGHTS = [2e4, 1e5, 1e3] # Checkpoint single style\n",
    "#STYLE_WEIGHTS = [5e4, 8e4, 3e4] # Checkpoint two styles\n",
    "LAMBDAS = [1., 0.5, 0.25]\n",
    "REG = 1e-7\n",
    "LOG_INTERVAL = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Allow PIL to read truncated blocks when loading images \"\"\"\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b7d18c7410>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Add a seed to have reproducable results \"\"\"\n",
    "\n",
    "SEED = 1080\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Configure training with or without cuda \"\"\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True}\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset..\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] El sistema no puede encontrar la ruta especificada: 'C:\\\\Users\\\\jorge\\\\Desktop\\\\Sketcher-AI\\\\models/../coco/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([transforms\u001b[39m.\u001b[39mResize(IMAGE_SIZE),\n\u001b[0;32m      7\u001b[0m                                 transforms\u001b[39m.\u001b[39mCenterCrop(IMAGE_SIZE),\n\u001b[0;32m      8\u001b[0m                                 transforms\u001b[39m.\u001b[39mToTensor(), tensor_normalizer()])\n\u001b[0;32m      9\u001b[0m \u001b[39m# http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m train_dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mImageFolder(DATASET, transform)\n\u001b[0;32m     11\u001b[0m \u001b[39m# http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader\u001b[39;00m\n\u001b[0;32m     12\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\datasets\\folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    302\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    303\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    310\u001b[0m         root,\n\u001b[0;32m    311\u001b[0m         loader,\n\u001b[0;32m    312\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    313\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[0;32m    314\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[0;32m    315\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[0;32m    316\u001b[0m     )\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\datasets\\folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    135\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    136\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    142\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 144\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[0;32m    145\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[0;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\datasets\\folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[0;32m    192\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \n\u001b[0;32m    194\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\datasets\\folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[0;32m     36\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[39m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[0;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[0;32m     42\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] El sistema no puede encontrar la ruta especificada: 'C:\\\\Users\\\\jorge\\\\Desktop\\\\Sketcher-AI\\\\models/../coco/'"
     ]
    }
   ],
   "source": [
    "\"\"\" Load coco dataset \"\"\"\n",
    "\n",
    "print(\"Loading dataset..\")\n",
    "scriptDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "DATASET = scriptDir + \"/../coco/\"\n",
    "transform = transforms.Compose([transforms.Resize(IMAGE_SIZE),\n",
    "                                transforms.CenterCrop(IMAGE_SIZE),\n",
    "                                transforms.ToTensor(), tensor_normalizer()])\n",
    "# http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder\n",
    "train_dataset = datasets.ImageFolder(DATASET, transform)\n",
    "# http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Load Style Image \"\"\"\n",
    "\n",
    "style_img_256, style_img_512, style_img_1024 = style_loader(\n",
    "    \"styles/\" + STYLE_NAME + \".jpg\", device, [256, 512, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(style_img_256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Define Loss Network \"\"\"\n",
    "\n",
    "StyleOutput = namedtuple(\"StyleOutput\", [\"relu1_1\", \"relu2_1\", \"relu3_1\", \"relu4_1\"])\n",
    "ContentOutput = namedtuple(\"ContentOutput\", [\"relu2_1\"])\n",
    "\n",
    "# https://discuss.pytorch.org/t/how-to-extract-features-of-an-image-from-a-trained-model/119/3\n",
    "class LossNetwork(torch.nn.Module):\n",
    "    def __init__(self, vgg):\n",
    "        super(LossNetwork, self).__init__()\n",
    "        self.vgg = vgg\n",
    "        self.layer_name_mapping = {\n",
    "            '1': \"relu1_1\", '3': \"relu1_2\",\n",
    "            '6': \"relu2_1\", '8': \"relu2_2\",\n",
    "            '11': \"relu3_1\", '13': \"relu3_2\", '15': \"relu3_3\", '17': \"relu3_4\",\n",
    "            '20': \"relu4_1\", '22': \"relu4_2\", '24': \"relu4_3\", '26': \"relu4_4\",\n",
    "            '29': \"relu5_1\", '31': \"relu5_2\", '33': \"relu5_3\", '35': \"relu5_4\"\n",
    "        }\n",
    "\n",
    "    def forward(self, x, mode):\n",
    "        if mode == 'style':\n",
    "            layers = ['1', '6', '11', '20']\n",
    "        elif mode == 'content':\n",
    "            layers = ['6']\n",
    "        else:\n",
    "            print(\"Invalid mode. Select between 'style' and 'content'\")\n",
    "        output = {}\n",
    "        for name, module in self.vgg._modules.items():\n",
    "            x = module(x)\n",
    "            if name in layers:\n",
    "                output[self.layer_name_mapping[name]] = x\n",
    "        if mode == 'style':\n",
    "            return StyleOutput(**output)\n",
    "        else:\n",
    "            return ContentOutput(**output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Load and extract features from VGG16 \"\"\"\n",
    "\n",
    "print(\"Loading VGG..\")\n",
    "vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "loss_network = LossNetwork(vgg).to(device).eval()\n",
    "del vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Before training, compute the features of every resolution of the style image \"\"\"\n",
    "\n",
    "print(\"Computing style features..\")\n",
    "with torch.no_grad(): \n",
    "    style_loss_features_256 = loss_network(Variable(style_img_256), 'style')\n",
    "    style_loss_features_512 = loss_network(Variable(style_img_512), 'style')\n",
    "    style_loss_features_1024 = loss_network(Variable(style_img_1024), 'style')\n",
    "gram_style_256 = [Variable(gram_matrix(y).data, requires_grad=False) for y in style_loss_features_256]\n",
    "gram_style_512 = [Variable(gram_matrix(y).data, requires_grad=False) for y in style_loss_features_512]\n",
    "gram_style_1024 = [Variable(gram_matrix(y).data, requires_grad=False) for y in style_loss_features_1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Init Net and loss \"\"\"\n",
    "\n",
    "style_subnet = StyleSubnet().to(device)\n",
    "enhance_subnet = EnhanceSubnet().to(device)\n",
    "refine_subnet = RefineSubnet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Prepare Training \"\"\"\n",
    "\n",
    "max_iterations = min(10000, len(train_dataset))\n",
    "\n",
    "# init loss\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "# init optimizer\n",
    "optimizer = torch.optim.Adam(list(style_subnet.parameters()) + \n",
    "                             list(enhance_subnet.parameters()) +\n",
    "                             list(refine_subnet.parameters()), lr=LR)\n",
    "\n",
    "def getLosses(generated_img, resized_input_img, content_weight, style_weight, mse_loss, gram_style):\n",
    "    \n",
    "    # Compute features\n",
    "    generated_style_features = loss_network(generated_img, 'style')\n",
    "    generated_content_features = loss_network(generated_img, 'content')\n",
    "    target_content_features = loss_network(resized_input_img, 'content')\n",
    "    \n",
    "    # Content loss\n",
    "    target_content_features = Variable(target_content_features[0].data, requires_grad=False)\n",
    "    content_loss = content_weight * mse_loss(generated_content_features[0], target_content_features)\n",
    "    \n",
    "    # Style loss\n",
    "    style_loss = 0.\n",
    "    for m in range(len(generated_style_features)):\n",
    "        gram_s = gram_style[m]\n",
    "        gram_y = gram_matrix(generated_style_features[m])\n",
    "        style_loss += style_weight * mse_loss(gram_y, gram_s.expand_as(gram_y))\n",
    "    \n",
    "    # Regularization loss\n",
    "    reg_loss = REG * (\n",
    "        torch.sum(torch.abs(generated_img[:, :, :, :-1] - generated_img[:, :, :, 1:])) + \n",
    "        torch.sum(torch.abs(generated_img[:, :, :-1, :] - generated_img[:, :, 1:, :])))\n",
    "    \n",
    "    return content_loss, style_loss, reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Perform Training \"\"\"\n",
    "\n",
    "style_subnet.train()\n",
    "enhance_subnet.train()\n",
    "refine_subnet.train()\n",
    "start = time.time()\n",
    "print(\"Start training on {}...\".format(device))\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    agg_content_loss, agg_style_loss, agg_reg_loss = 0., 0., 0.\n",
    "    log_counter = 0\n",
    "    for i, (x, _) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        # update learning rate every 2000 iterations\n",
    "        if i % 2000 == 0 and i != 0:\n",
    "            LR = LR * 0.8\n",
    "            optimizer = torch.optim.Adam(list(style_subnet.parameters()) + \n",
    "                                         list(enhance_subnet.parameters()) +\n",
    "                                         list(refine_subnet.parameters()), lr=LR)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x_in = x.clone()\n",
    "        \n",
    "        \"\"\" Style Subnet \"\"\"\n",
    "        x_in = Variable(x_in).to(device)\n",
    "\n",
    "        # Generate image\n",
    "        generated_img_256, resized_input_img_256 = style_subnet(x_in)\n",
    "        resized_input_img_256 = Variable(resized_input_img_256.data)\n",
    "        \n",
    "        # Compute Losses\n",
    "        style_subnet_content_loss, style_subnet_style_loss, style_subnet_reg_loss = getLosses(\n",
    "            generated_img_256,\n",
    "            resized_input_img_256,\n",
    "            CONTENT_WEIGHTS[0],\n",
    "            STYLE_WEIGHTS[0],\n",
    "            mse_loss, gram_style_256)\n",
    "        \n",
    "        \"\"\" Enhance Subnet \"\"\"\n",
    "        x_in = Variable(generated_img_256)\n",
    "        \n",
    "        # Generate image\n",
    "        generated_img_512, resized_input_img_512 = enhance_subnet(x_in)\n",
    "        resized_input_img_512 = Variable(resized_input_img_512.data)\n",
    "        \n",
    "        # Compute Losses\n",
    "        enhance_subnet_content_loss, enhance_subnet_style_loss, enhance_subnet_reg_loss = getLosses(\n",
    "            generated_img_512,\n",
    "            resized_input_img_512,\n",
    "            CONTENT_WEIGHTS[1],\n",
    "            STYLE_WEIGHTS[1],\n",
    "            mse_loss, gram_style_512)\n",
    "        \n",
    "        \"\"\" Refine Subnet \"\"\"\n",
    "        x_in = Variable(generated_img_512)\n",
    "        \n",
    "        # Generate image\n",
    "        generated_img_1024, resized_input_img_1024 = refine_subnet(x_in)\n",
    "        resized_input_img_1024 = Variable(resized_input_img_1024.data)\n",
    "        \n",
    "        # Compute Losses\n",
    "        refine_subnet_content_loss, refine_subnet_style_loss, refine_subnet_reg_loss = getLosses(\n",
    "            generated_img_1024,\n",
    "            resized_input_img_1024,\n",
    "            CONTENT_WEIGHTS[2],\n",
    "            STYLE_WEIGHTS[2],\n",
    "            mse_loss, gram_style_1024)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = LAMBDAS[0] * (style_subnet_content_loss + style_subnet_style_loss + style_subnet_reg_loss) + \\\n",
    "                     LAMBDAS[1] * (enhance_subnet_content_loss + enhance_subnet_style_loss + enhance_subnet_reg_loss) + \\\n",
    "                     LAMBDAS[2] * (refine_subnet_content_loss + refine_subnet_style_loss + refine_subnet_reg_loss)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Aggregated loss\n",
    "        agg_content_loss += style_subnet_content_loss.data[0] + \\\n",
    "                            enhance_subnet_content_loss.data[0] + \\\n",
    "                            refine_subnet_content_loss.data[0]\n",
    "        agg_style_loss += style_subnet_style_loss.data[0] + \\\n",
    "                          enhance_subnet_style_loss.data[0] + \\\n",
    "                          refine_subnet_style_loss.data[0]\n",
    "        \n",
    "        agg_reg_loss += style_subnet_reg_loss.data[0] + \\\n",
    "                        enhance_subnet_reg_loss.data[0] + \\\n",
    "                        refine_subnet_reg_loss.data[0]\n",
    "        \n",
    "        \n",
    "        # log training process\n",
    "        if (i + 1) % LOG_INTERVAL == 0:\n",
    "            log_counter += 1\n",
    "            hlp = log_counter * LOG_INTERVAL\n",
    "            time_per_pass = (time.time() - start) / hlp\n",
    "            estimated_time_left = (time_per_pass * (max_iterations - i))/3600\n",
    "            print(\"{} [{}/{}] time per pass: {:.2f}s  total time: {:.2f}s  estimated time left: {:.2f}h  content: {:.6f}  style: {:.6f}  reg: {:.6f}  total: {:.6f}\".format(\n",
    "                        time.ctime(), i+1, max_iterations,\n",
    "                        (time.time() - start) / hlp,\n",
    "                        time.time() - start,\n",
    "                        estimated_time_left,\n",
    "                        agg_content_loss / LOG_INTERVAL,\n",
    "                        agg_style_loss / LOG_INTERVAL,\n",
    "                        agg_reg_loss / LOG_INTERVAL,\n",
    "                        (agg_content_loss + agg_style_loss + agg_reg_loss) / LOG_INTERVAL))\n",
    "            agg_content_loss, agg_style_loss, agg_reg_loss = 0., 0., 0.\n",
    "            imshow(x, title=\"input image\")\n",
    "            imshow(generated_img_256, title=\"generated_img_256\")\n",
    "            imshow(generated_img_512, title=\"generated_img_512\")\n",
    "            imshow(generated_img_1024, title=\"generated_img_1024\")\n",
    "\n",
    "        \"\"\"\n",
    "        if (i + 1) % (10 * LOG_INTERVAL) == 0:\n",
    "            save_image(generated_img_256, title=\"log_data/256_iteration_{}_of_{}\".format(i+1, max_iterations))\n",
    "            save_image(generated_img_512, title=\"log_data/512_iteration_{}_of_{}\".format(i+1, max_iterations))\n",
    "            save_image(generated_img_1024, title=\"log_data/1024_iteration_{}_of_{}\".format(i+1, max_iterations))\n",
    "            torch.save(style_subnet, 'log_data/trained_style_subnet_it_{}_of_{}.pt'.format(i+1, max_iterations))\n",
    "            torch.save(enhance_subnet, 'log_data/trained_enhance_subnet_it_{}_of_{}.pt'.format(i+1, max_iterations))\n",
    "            torch.save(refine_subnet, 'log_data/trained_refine_subnet_it_{}_of_{}.pt'.format(i+1, max_iterations))\n",
    "            print(\"Images and models saved in /log_one_data\")\n",
    "        \"\"\"\n",
    "\n",
    "        # Stop training after max iterations\n",
    "        if (i + 1) == max_iterations: break\n",
    "\n",
    "\"\"\" Save model \"\"\"\n",
    "torch.save(style_subnet, 'models/trained_style_subnet.pt')\n",
    "torch.save(enhance_subnet, 'models/trained_enhance_subnet.pt')\n",
    "torch.save(refine_subnet, 'models/trained_refine_subnet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
